{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install hydra-core\n",
        "%pip install pytorch-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVERLbDwV133",
        "outputId": "47c0ae3b-a006-416a-bef3-be1158591dc8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from hydra-core) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core) (24.1)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.1)\n",
            "Collecting pytorch-lightning\n",
            "  Using cached pytorch_lightning-2.3.3-py3-none-any.whl (812 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.25.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.4)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Using cached torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Using cached lightning_utilities-0.11.3.post0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0.0->pytorch-lightning)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0.0->pytorch-lightning)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0.0->pytorch-lightning)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.0.0->pytorch-lightning)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0.0->pytorch-lightning)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0.0->pytorch-lightning)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0.0->pytorch-lightning)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0.0->pytorch-lightning)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0.0->pytorch-lightning)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.0.0->pytorch-lightning)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0.0->pytorch-lightning)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->pytorch-lightning)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pytorch-lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.11.3.post0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 pytorch-lightning-2.3.3 torchmetrics-1.4.0.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fPjx-2NWMNU1"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import hydra\n",
        "from omegaconf import DictConfig, OmegaConf\n",
        "from pytorch_lightning import (\n",
        "    Callback,\n",
        "    LightningDataModule,\n",
        "    LightningModule,\n",
        "    Trainer,\n",
        "    seed_everything,\n",
        ")\n",
        "from pytorch_lightning.loggers import Logger\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from src.utils import utils\n",
        "# from src.models.OneModel import OneModel\n",
        "\n",
        "# log = utils.get_logger(__name__)\n",
        "\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wwZkKqGVyS7",
        "outputId": "ceef7f91-d1a2-4996-f7cf-35c057885ba2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# where to download the datasets\n",
        "data_dir = \"/path/to/dir/\"\n",
        "\n",
        "# where to upload the weights and biases logs\n",
        "my_project = \"tutorial_notebook\"\n",
        "my_entity = \"xyz\""
      ],
      "metadata": {
        "id": "fV0xrcxPcZb1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"trainer\": {\n",
        "        \"_target_\": \"pytorch_lightning.Trainer\",\n",
        "        \"gpus\": 1,\n",
        "        \"min_epochs\": 1,\n",
        "        \"max_epochs\": 100,\n",
        "        \"weights_summary\": None,\n",
        "        \"progress_bar_refresh_rate\": 20,\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"_target_\": \"src.models.OneModel.OneModel\",\n",
        "        \"model\": {\n",
        "            \"_target_\": \"src.models.modules.resnet_cifar.ResNet18\"\n",
        "        },\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"_target_\": \"torch.optim.AdamW\",\n",
        "        \"lr\": 0.001\n",
        "    },\n",
        "    \"datamodule\": {\n",
        "        \"_target_\": \"src.datamodules.datamodules.CIFAR10DataModule\",\n",
        "        \"data_dir\": data_dir,\n",
        "        \"batch_size\": 320,\n",
        "        \"num_workers\": 4,\n",
        "        \"pin_memory\": True,\n",
        "        \"shuffle\": True,\n",
        "        \"trainset_data_aug\": False,\n",
        "        # This is the irreducible loss model training, so we train on the\n",
        "        # holdout set (we call this set the \"valset\" in the global terminology for the dataset\n",
        "        # splits). Thus, we need augmentation on the valset\n",
        "        \"valset_data_aug\": True,\n",
        "    },\n",
        "    \"callbacks\": {\n",
        "        # We want to save that irreducible loss model with the lowest validation\n",
        "        # loss (we validate on the \"trainset\", in global terminology for the\n",
        "        # dataset splits).\n",
        "        \"model_checkpoint\": {\n",
        "            \"_target_\": \"pytorch_lightning.callbacks.ModelCheckpoint\",\n",
        "            \"monitor\": \"val_loss_epoch\",\n",
        "            \"mode\": \"min\",\n",
        "            \"save_top_k\": 1,\n",
        "            \"save_last\": True,\n",
        "            \"verbose\": False,\n",
        "            \"dirpath\": os.path.join(\"tutorial_outputs\", \"irreducible_loss_model\"),\n",
        "            \"filename\": \"epoch_{epoch:03d}\",\n",
        "            \"auto_insert_metric_name\": False,\n",
        "        },\n",
        "    },\n",
        "    \"logger\": {\n",
        "        # Log with wandb, you could choose a different logger\n",
        "        \"wandb\": {\n",
        "            \"_target_\": \"pytorch_lightning.loggers.wandb.WandbLogger\",\n",
        "            \"project\": my_project,\n",
        "            \"save_dir\": \".\",\n",
        "            \"entity\": my_entity,\n",
        "            \"job_type\": \"train\",\n",
        "        }\n",
        "    },\n",
        "    \"seed\": 12,\n",
        "    \"debug\": False,\n",
        "    \"ignore_warnings\": True,\n",
        "    \"test_after_training\": True,\n",
        "    \"base_outdir\": \"logs\",\n",
        "}"
      ],
      "metadata": {
        "id": "upVKdKZocc8X"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert config to OmegaConf structured dict (default for Hydra), and pretty-print\n",
        "config = OmegaConf.create(config)"
      ],
      "metadata": {
        "id": "jcpUNgDici8r"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for random number generators in pytorch, numpy and python.random\n",
        "if \"seed\" in config:\n",
        "    seed_everything(config.seed, workers=True)\n",
        "\n",
        "# Init lightning datamodule\n",
        "print(f\"Instantiating datamodule <{config.datamodule._target_}>\")\n",
        "datamodule: LightningDataModule = hydra.utils.instantiate(config.datamodule)\n",
        "datamodule.setup()\n",
        "\n",
        "# Init lightning model\n",
        "print(f\"Instantiating model <{config.model._target_}>\")\n",
        "pl_model: LightningModule = hydra.utils.instantiate(\n",
        "    config=config.model,\n",
        "    optimizer_config=utils.mask_config(\n",
        "        config.get(\"optimizer\", None)\n",
        "    ),  # When initialising the optimiser, you need to pass it the model parameters. As we haven't initialised the model yet, we cannot initialise the optimizer here. Thus, we need to pass-through the optimizer-config, to initialise it later. However, hydra.utils.instantiate will instatiate everything that looks like a config (if _recursive_==True, which is required here bc OneModel expects a model argument). Thus, we \"mask\" the optimizer config from hydra, by modifying the dict so that hydra no longer recognises it as a config.\n",
        "    scheduler_config=utils.mask_config(\n",
        "        config.get(\"scheduler\", None)\n",
        "    ),  # see line above\n",
        "    datamodule=datamodule,\n",
        "    _convert_=\"partial\",\n",
        ")\n",
        "\n",
        "# Init lightning callbacks. Here, we only use one callback: saving the model\n",
        "# with the lowest validation set loss.\n",
        "callbacks: List[Callback] = []\n",
        "if \"callbacks\" in config:\n",
        "    for _, cb_conf in config.callbacks.items():\n",
        "        if \"_target_\" in cb_conf:\n",
        "            print(f\"Instantiating callback <{cb_conf._target_}>\")\n",
        "            callbacks.append(hydra.utils.instantiate(cb_conf))\n",
        "\n",
        "# Init lightning loggers. Here, we use wandb.\n",
        "logger: List[LightningLoggerBase] = []\n",
        "if \"logger\" in config:\n",
        "    for _, lg_conf in config.logger.items():\n",
        "        if \"_target_\" in lg_conf:\n",
        "            print(f\"Instantiating logger <{lg_conf._target_}>\")\n",
        "            logger.append(hydra.utils.instantiate(lg_conf))\n",
        "\n",
        "# Init lightning trainer\n",
        "print(f\"Instantiating trainer <{config.trainer._target_}>\")\n",
        "trainer: Trainer = hydra.utils.instantiate(\n",
        "    config.trainer, callbacks=callbacks, logger=logger, _convert_=\"partial\"\n",
        ")\n",
        "\n",
        "# Send config to all lightning loggers\n",
        "print(\"Logging hyperparameters!\")\n",
        "trainer.logger.log_hyperparams(config)\n",
        "\n",
        "# Train the model.\n",
        "print(\"Starting training!\")\n",
        "trainer.fit(\n",
        "    pl_model,\n",
        "    train_dataloaders=datamodule.val_dataloader(), # see Markdown comment above\n",
        "    val_dataloaders=datamodule.train_dataloader(), # see Markdown comment above\n",
        ")\n",
        "\n",
        "# Evaluate model on test set, using the best model achieved during training\n",
        "if config.get(\"test_after_training\") and not config.trainer.get(\"fast_dev_run\"):\n",
        "    print(\"Starting testing!\")\n",
        "    trainer.test(test_dataloaders=datamodule.test_dataloader())\n",
        "\n",
        "def evaluate_and_save_model_from_checkpoint_path(checkpoint_path, name):\n",
        "    \"\"\"Compute irreducible loss for the whole trainset with the best model\"\"\"\n",
        "\n",
        "    # load best model\n",
        "    model = OneModel.load_from_checkpoint(checkpoint_path)\n",
        "\n",
        "    # compute irreducible losses\n",
        "    model.eval()\n",
        "    irreducible_loss_and_checks = utils.compute_losses_with_sanity_checks(\n",
        "        dataloader=datamodule.train_dataloader(), model=model\n",
        "    )\n",
        "\n",
        "    # save irred losses in same directory as model checkpoint\n",
        "    path = os.path.join(\n",
        "        os.path.dirname(trainer.checkpoint_callback.best_model_path),\n",
        "        name,\n",
        "    )\n",
        "    torch.save(irreducible_loss_and_checks, path)\n",
        "\n",
        "    return path\n",
        "\n",
        "saved_path = evaluate_and_save_model_from_checkpoint_path(\n",
        "    trainer.checkpoint_callback.best_model_path, \"irred_losses_and_checks.pt\"\n",
        ")\n",
        "\n",
        "print(f\"Using monitor: {trainer.checkpoint_callback.monitor}\")\n",
        "\n",
        "# Print path to best checkpoint\n",
        "print(f\"Best checkpoint path:\\n{trainer.checkpoint_callback.best_model_path}\")\n",
        "print(f\"Best checkpoint irred_losses_path:\\n{saved_path}\")\n",
        "\n",
        "\n",
        "# Make sure everything closed properly\n",
        "log.info(\"Finalizing!\")\n",
        "utils.finish(\n",
        "    config=config,\n",
        "    model=pl_model,\n",
        "    datamodule=datamodule,\n",
        "    trainer=trainer,\n",
        "    callbacks=callbacks,\n",
        "    logger=logger,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "x0H_jjo-cl3h",
        "outputId": "e92f1ad5-0538-417c-abbf-30cdde64e055"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instantiating datamodule <src.datamodules.datamodules.CIFAR10DataModule>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InstantiationException",
          "evalue": "Error locating target 'src.datamodules.datamodules.CIFAR10DataModule', set env var HYDRA_FULL_ERROR=1 to see chained exception.\nfull_key: datamodule",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\u001b[0m in \u001b[0;36m_locate\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc_import\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hydra/_internal/instantiate/_instantiate2.py\u001b[0m in \u001b[0;36m_resolve_target\u001b[0;34m(target, full_key)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_locate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\u001b[0m in \u001b[0;36m_locate\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc_import\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m    638\u001b[0m             \u001b[0;34mf\"Error loading '{path}':\\n{repr(exc_import)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Error loading 'src.datamodules.datamodules.CIFAR10DataModule':\nModuleNotFoundError(\"No module named 'src'\")\nAre you sure that module 'src' is installed?",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mInstantiationException\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d13e5bb46cd3>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Init lightning datamodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Instantiating datamodule <{config.datamodule._target_}>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdatamodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLightningDataModule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstantiate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hydra/_internal/instantiate/_instantiate2.py\u001b[0m in \u001b[0;36minstantiate\u001b[0;34m(config, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0m_partial_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPARTIAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         return instantiate_node(\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_recursive_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_convert_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_partial_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hydra/_internal/instantiate/_instantiate2.py\u001b[0m in \u001b[0;36minstantiate_node\u001b[0;34m(node, convert, recursive, partial, *args)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mexclude_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"_target_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_convert_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_recursive_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_partial_\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0m_target_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_resolve_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTARGET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mis_partial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_partial_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hydra/_internal/instantiate/_instantiate2.py\u001b[0m in \u001b[0;36m_resolve_target\u001b[0;34m(target, full_key)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfull_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\nfull_key: {full_key}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInstantiationException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Expected a callable target, got '{target}' of type '{type(target).__name__}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInstantiationException\u001b[0m: Error locating target 'src.datamodules.datamodules.CIFAR10DataModule', set env var HYDRA_FULL_ERROR=1 to see chained exception.\nfull_key: datamodule"
          ]
        }
      ]
    }
  ]
}